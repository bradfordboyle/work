\documentclass[10pt]{article}
\usepackage{acronym}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{Steven}
\usepackage{subfigure}

%\newtheorem{proposition}{Proposition}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}

\oddsidemargin -0.5in
\evensidemargin -0.5in
\textwidth 7.5in
\topmargin -0.5in
\textheight 9in

\begin{document}
\tableofcontents
\listoffigures
\section*{Acronyms Used In This Document}
\begin{acronym}
	\acro{MC}{Monte Carlo}
	\acro{AV}{antithetic variables}
	\acro{PAV}{parameterized variables}
	\acro{CRLB}{Cramer Rao Lower Bound}
	\acro{PDF}{probability density function}
	\acro{MVU}{minimum variance unbiased}
\end{acronym}

\setcounter{section}{-1}
\section{Work In Progress}
\subsection{To Do}
\paragraph{CRLB}
Read up on the \ac{CRLB} and see how it applies to our estimator. How close is our estimator to the \ac{CRLB}?

\paragraph{Equivalent}
Reduce parallel/series subsystems to a single component that has the same probability of working. Match the mean. Reading from book borrowed from Dr. Weber.

\paragraph{Generalize AV/PAV}
This document contains our work on \ac{PAV} in terms of Bernoulli random variables. Work out for the single link case with exponential random variables and use that to help generalize our work in terms of a generic function $h(\cdot)$

\paragraph{Model Justification}
Reread the \cite{ni-2008}. Find physical justification for our setup.

\subsection{Notes}
The difficulty with system reliability determination is in enumerating the paths from source to sink, which grows as the number of components (edges) increases. For determining this we can use simulation.

\paragraph{Monte Carlo}
We represent the system as a graph $G(\Vca,\Eca)$, with an edge for each component. Two edges are incident with each other if the corresponding components are connected. Each component either works or it doesn't. Generate a sequence of uniform random variables, one for each edge and assume independent failure of components. We construct a subgraph by taking edges that are on ($U_i < p_i$). If the resulting subgraph is connected, then the system works.
\[
\frac{1}{n}\displaystyle\sum_{i=1}^{n}\Ibb[\exists (s-t)~\mbox{path in}~G_i]
\]
This estimator is unbiased
\[
\Ebb[\hat{r}]=\Pbb[\exists (s-t)~\mbox{path in}~G]=r
\]

\paragraph{Antithetic Variables}
Same idea as with \ac{MC}, except after forming a subgraph based on $U_i$'s, we form the compliment subgraph based on $1-U_i$'s.
\[
\hat{r}_{AV}=\frac{1}{n}\displaystyle\sum_{i=1}^{n/2}\Ibb[\exists (s-t)~\mbox{path in}~G_i]+\Ibb[\exists (s-t)~\mbox{path in}~G_i^c]
\]
This estimator is unbiased as well
\[
\Ebb[\hat{r}]=\frac{1}{2}\left[\Pbb[\exists (s-t)~\mbox{path in}~G]+\Pbb[\exists (s-t)~\mbox{path in}~G]\right]=r
\]
We know that $\Ibb[\exists (s-t)~\mbox{path in}~G]$ can be written as a function of the underlying $U_i$'s ($h(U_1,\ldots,U_k)$) and because $U$ and $1-U$ are identically distributed we know that $\Pbb[\exists (s-t)~\mbox{path in}~G]=\Pbb[\exists (s-t)~\mbox{path in}~G^c]$.

\section{Single Link}
We consider the case where a single source is transmitting a sequence of packets to a single destination (Figure~\ref{fig:single_link}) and the source records the outcome ($X_{1}^{(n)}$ of the packet. Examples of outcomes are loss information or delay information; we consider loss based tomography. In this case, the outomes are Bernoulli random variables, with $X_{1}^{(n)}=1$ with probability $\alpha$ denoting successful reception of the packet. We are interested in developing an unbiased estimator for the link success probability $\alpha$ with a low variance.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.4\columnwidth]{img/single_link}
\caption{Single Link}\label{fig:single_link}
\end{figure}

\subsection{Monte Carlo Estimation}
The simplest estimation technique is \ac{MC}. We generate a sequence of unifrom random variables $U_{1}^{(n)}$ and transforming them to Bernoulli random variables $X_{1}^{(n)} = h(U_{1}^{(n)}) = \mathbb{I}_{U_{1}^{(n)} < \alpha}$. The \ac{MC} estimator is then

\begin{equation}
\hat{\alpha}_{MC} = \frac{1}{n}\displaystyle\sum_{i=1}^{n}X_{1}^{(i)}
\end{equation}

This is clearly unbiased and has a normalized variance of

\begin{equation}
n\var\left(\hat{\alpha}_{MC}\right) = \alpha(1-\alpha)
\end{equation}

The variance as a function of $\alpha$ is shown in Figure~\ref{fig:mc}. The variance (or mean squared error since the estimator is ubiased) is maximum at $\alpha = \frac{1}{2}$.
\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\columnwidth]{img/monte_carlo}
\caption[Normalized variance for \acs{MC} as a function of link success probability, $\alpha$]{Normalized variance for \acf{MC} as a function of link success probability, $\alpha$}\label{fig:mc}
\end{figure}

\subsection{Antithetic Variables}
Our first step towards improving over the simple \ac{MC} estimation is the use of \ac{AV}\cite{ross-2006}. The idea here is that instead of generating a sequence with $n$ random variables, we generate a sequence of length $\frac{n}{2}$ and form the estimator

\begin{equation}
\hat{\alpha}_{AV} = \frac{1}{n}\displaystyle\sum_{i=1}^{n/2}h(U_{1}^{(i)})+h(1 - U_{1}^{(i)})
\end{equation}

Because expectation is linear and because $h(U_{1}^{(n)})$ and $h(1-U_{1}^{(i)})$ are identically distributed, it is easy enough to verify that the estimator is unbiased. The normalized variance is given as

\begin{align*}
n\var\left(\hat{\alpha}_{AV}\right) &= \frac{1}{n}\sum_{i=1}^{n/2}\var\left(h(U_{1}^{(i)}) + h(1-U_{1}^{(i)})\right)\\
&=\frac{\var\left(h(U_{1}^{(i)})\right) + \var\left(h(1-U_{1}^{(i)})\right) + 2\cov\left(h(U_{1}^{(i)}),h(1-U_{1}^{(i)})\right)}{2}\\
&=\var\left(h(U_{1}^{(i)})\right) + \cov\left(h(U_{1}^{(i)}),h(1-U_{1}^{(i)})\right)\\
&=\mathbb{E}\left[h(U_{1}^{(i)})^2\right] + \mathbb{E}\left[h(U_{1}^{(i)})h(1-U_{1}^{(i)})\right] - 2\mathbb{E}\left[h(U_{1}^{(i)})\right]^{2}\\
&=\alpha+ 2\mathbb{I}_{\alpha > \frac{1}{2}}(\alpha-\frac{1}{2})-2\alpha^{2}
\end{align*}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\columnwidth]{img/av}
\caption[Normalized variance for \acs{AV} as a function of link success probability, $\alpha$]{Normalized variance for \acf{AV} as a function of link success probability, $\alpha$}\label{fig:av}
\end{figure}

The normalized variance as a function of $\alpha$ is shown in Figure~\ref{fig:av}. Note that the normalized variance is maximum at $\alpha = \frac{1}{4}$ and $\alpha = \frac{3}{4}$. From this figure we observe:
\begin{proposition}\label{prop:av}
\[
\var\left(\hat{\alpha}_{AV}\right) < \var\left(\hat{\alpha}_{MC}\right), \forall \alpha \in (0,1)
\]
\end{proposition}

\begin{proof}
First, consider when $\alpha < \frac{1}{2}$
\[
\alpha - 2\alpha^2  <  \alpha(1-\alpha)
\]
\[
\alpha - 2\alpha^2  <  \alpha-\alpha^2
\]
\[
0  <  \alpha^2
\]
Next consider when $\alpha > \frac{1}{2}$
\[
3\alpha - 1 - 2\alpha^2 < \alpha - \alpha^{2}
\]
\[
0 < \alpha^2 - 2\alpha + 1
\]
\[
0 < (\alpha - 1)^2
\]
Finally, when $\alpha = \frac{1}{2}$ we have $\var\left(\hat{\alpha}_{AV}\right) = 0$ and $\var\left(\hat{\alpha}_{MC}\right) = \frac{1}{4}$
\end{proof}
Using \ac{AV}, we have the greatest variance reduction when the variance of the \ac{MC} estimator is at its maximum. The closer $\alpha$ is to either $0$ or $1$, there is less variance reduction.

\subsection{Parameterized Antithetic Variables}
We wish to develop an estimator with a tunable parameter $\beta$ such that the variance reduction is maximized when $\beta=\alpha$, but which is an unbiased estimator for all $\alpha$ and all $\beta$.

Consider $\alpha$ fixed and unknown and to be estimated, and $\beta \in [0,1]$ chosen based on some prior assumptions about $\alpha$. Generate $U_{1}^{(n)} \sim \mathrm{Uni}(0,\beta)$ and from there set
\begin{equation}
\bar{U}_{1}^{(n)} = \frac{1-\beta}{\beta} U_1 + \beta.
\end{equation}
Note that $\bar{U}_{1}^{(n)} \sim \mathrm{Uni}(\beta,1)$. We consider an estimator of the form
\begin{equation}
\hat{\alpha}_{PAV} = \frac{1}{n} \sum_{i=1}^{n/2} \left( \beta \left( \mathbf{1}_{U_{1}^{(i)} \leq \alpha} + \mathbf{1}_{\beta-U_{1}^{(i)} \leq \alpha} \right) + (1-\beta) \left( \mathbf{1}_{\bar{U}_{1}^{(i)} \leq \alpha} + \mathbf{1}_{1-\bar{U}_{1}^{(i)} + \beta < \alpha} \right) \right)
\end{equation}
Then it is straightforward to show
\begin{eqnarray}
\Ebb[\hat{\alpha}_{PAV}] &=& \frac{1}{2} \left[ \beta \left( \Pbb(U_1 \leq \alpha) + 1 - \Pbb(U_1 \leq \beta - \alpha) \right) + (1-\beta) \left( \Pbb(\bar{U}_1 \leq \alpha) + 1 - \Pbb( \bar{U}_1 \leq 1+ \beta - \alpha) \right) \right] \nonumber \\
&=& \left\{ \begin{array}{ll}
\frac{1}{2} \left[ \beta \left( \frac{\alpha}{\beta} + 1 - \frac{\beta-\alpha}{\beta} \right) + (1-\beta) \left(0 + 1 - 1 \right) \right] , \; & \alpha \leq \beta \\
\frac{1}{2} \left[ \beta \left(1 + 1 - 0 \right) + (1-\beta) \left( \frac{\alpha-\beta}{1-\beta} + 1 - \frac{1+\beta-\alpha-\beta}{1-\beta} \right) \right], \; & \alpha > \beta 
\end{array} \right. \nonumber \\
&=& \alpha
\end{eqnarray}

We can then find the normalized variance as
\begin{equation}n\var(\hat{\alpha}_{PAV}) = \left\{
\begin{array}{llll}
(\alpha-\beta)(\beta-(2\alpha-1)), & \alpha \leq \frac{1}{2} & \beta \leq \alpha & \\
(\beta-\alpha)(2\alpha-\beta), & & \alpha < \beta \leq 2 \alpha &  \\
\alpha(\beta-2\alpha), & & \beta > 2 \alpha & \\
(1-\alpha)((2\alpha-1)-\beta), & \alpha > \frac{1}{2} & \beta \leq \alpha & \beta \leq 2 \alpha - 1 \\
(\alpha-\beta)(\beta - (2\alpha-1)), & & \beta \leq \alpha & 2 \alpha - 1 < \beta \\
(\beta-\alpha)(2\alpha-\beta), & & \beta > \alpha 
\end{array}\right.
\end{equation}

The normalized variance for both \ac{AV} and \ac{PAV} is shown in Figure~\ref{fig:pav}.
\begin{figure}[ht!]
\centering
\subfigure[$\alpha = 0.2$]{
\includegraphics[width=0.4\columnwidth]{img/pav_2}
\label{fig:subfig1}
}
\subfigure[$\alpha = 0.4$]{
\includegraphics[width=0.4\columnwidth]{img/pav_4}
\label{fig:subfig2}
}
\subfigure[$\alpha = 0.6$]{
\includegraphics[width=0.4\columnwidth]{img/pav_6}
\label{fig:subfig3}
}
\subfigure[$\alpha = 0.8$]{
\includegraphics[width=0.4\columnwidth]{img/pav_8}
\label{fig:subfig4}
}
\label{fig:subfigureExample}
\caption[Normalized variance for \acs{PAV}]{Normalized variance for \acf{PAV}: \subref{fig:subfig1} $\alpha = 0.2$, \subref{fig:subfig2} $\alpha = 0.4$, \subref{fig:subfig3} $\alpha = 0.6$, and \subref{fig:subfig4} $\alpha = 0.8$}\label{fig:pav}
\end{figure}

\begin{proposition}\label{prop:pav}
\[
\var\left(\hat{\alpha}_{PAV}\right) < \var\left(\hat{\alpha}_{MC}\right), \forall \alpha,\beta \in (0,1)
\]
\end{proposition}
\begin{proof}
If $\alpha < \frac{1}{3}$, then $\displaystyle\max_{\beta}\var\left(\hat{\alpha}_{PAV}\right)=\alpha-2\alpha^2$; we are doing no worse than \ac{AV}. We proved in Proposition~\ref{prop:av} that \ac{AV} is always better than \ac{MC}. If $\frac{1}{3} < \alpha < \frac{1}{2}$, then $\displaystyle\max_{\beta}\var\left(\hat{\alpha}_{PAV}\right)=\frac{(1-\alpha)^2}{4}$.
\[
\frac{(1-\alpha)^2}{4} < \alpha(1-\alpha)
\]
\[
\frac{(1-\alpha)}{4} < \alpha
\]
\[
1-\alpha < 4\alpha < 4\frac{1}{2}
\]
\[
1-\alpha < 2
\]
Because $\alpha\in[0,1]$, the above is true and we conclude that $\var\left(\hat{\alpha}_{PAV}\right) < \var\left(\hat{\alpha}_{AV}\right)$ for $\alpha < \frac{1}{2}$.

If $\alpha > \frac{2}{3}$, then  $\displaystyle\max_{\beta}\var\left(\hat{\alpha}_{PAV}\right)=\alpha+2(\alpha-\frac{1}{2})-2\alpha^2$; again we are doing no worse than \ac{AV}. If $\frac{1}{3} < \alpha < \frac{1}{2}$, then $\displaystyle\max_{\beta}\var\left(\hat{\alpha}_{PAV}\right)=\frac{\alpha^2}{4}$.
\[
\frac{\alpha^2}{4} < \alpha(1-\alpha)
\]
\[
\frac{\alpha}{4} < 1-\alpha
\]
\[
\alpha < 4(1-\alpha) < 4\frac{1}{2}
\]
\[
\alpha < 2
\]
Because $\alpha\in[0,1]$, the above is true and we conclude that $\var\left(\hat{\alpha}_{PAV}\right) < \var\left(\hat{\alpha}_{AV}\right)$ for $\alpha > \frac{1}{2}$.

\end{proof}
\begin{corollary}
If $\alpha < \frac{1}{3}$ or $\alpha > \frac{2}{3}$, then
\[
\var\left(\hat{\alpha}_{PAV}\right) < \var\left(\hat{\alpha}_{AV}\right), \forall \beta \in (0,1)
\]
\end{corollary}

\begin{corollary}
If $\beta \geq 3\alpha -1,~\alpha \in (\frac{1}{3},\frac{1}{2})$ or $\beta \leq 3\alpha -1,~\alpha \in (\frac{1}{2},\frac{2}{3})$, then
\[
\var\left(\hat{\alpha}_{PAV}\right) < \var\left(\hat{\alpha}_{AV}\right)
\]
\end{corollary}

\begin{corollary}
If $\alpha \in (\frac{1}{3},\frac{1}{2})$ and $\beta < 3\alpha -1$, then the worst case choice of $\beta$ is
\[
\frac{3\alpha - 1}{2}
\]
which gives
\[
\var\left(\hat{\alpha}_{PAV}\right) =\left(\frac{1-\alpha}{2}\right)^2
\]
If $\alpha \in (\frac{1}{2},\frac{2}{3})$ and $\beta > 3\alpha -1$, then the worst case choice of $\beta$ is
\[
\frac{3\alpha}{2}
\]
which gives
\[
\var\left(\hat{\alpha}_{PAV}\right) =\left(\frac{\alpha}{2}\right)^2
\]
\end{corollary}

The ratio of \ac{PAV} variance to \ac{AV} variance is plotted in Figure~\ref{fig:ratio}.

\begin{figure}
\centering
\includegraphics[width=0.5\columnwidth]{img/ratio}
\caption[Ratio of \acs{PAV} to \acs{AV} as a function of link success probability, $\alpha$]{Ratio of \acf{PAV} to \acf{AV} as a function of link success probability, $\alpha$}\label{fig:ratio}
\end{figure}

\subsection{Cramer Rao Lower Bound}
If we let $m = \displaystyle\sum_{i=1}^nX_1^{(i)}$ denote the number of successful packet receptions, we can then write
\begin{equation}
p(\xbf;\alpha)=\displaystyle\prod_{i=1}^{m}\alpha\prod_{i=m+1}^{n}(1-\alpha)\label{eq:pdf}
\end{equation}
We first check to make sure the assumed \ac{PDF} meets the condition that
\begin{equation}
\Ebb\left[\frac{\partial \ln p(\xbf;\alpha)}{\partial\alpha}\right]=0,~\forall \alpha
\end{equation}

This is known as the ``regularity'' condition.
\[
\Ebb\left[\frac{\partial \left(m\ln\alpha + (m-n)\ln(1-\alpha)\right)}{\partial\alpha}\right]=\Ebb\left[\frac{m}{\alpha}+\frac{(m-n)}{1-\alpha}\right]
\]
\[
\frac{1}{\alpha}\Ebb\left[m\right]+\frac{1}{1-\alpha}\Ebb\left[m\right]-\frac{N}{1-\alpha}
\]
\[
\frac{\Ebb[m]}{\alpha(1-\alpha)}-\frac{N}{1-\alpha}=\frac{N}{1-\alpha}-\frac{N}{1-\alpha}
\]
The assumed \ac{PDF} meets the regularity condition and therefore the variance of any unbiased estimator $\hat{\alpha}$ must satisfy
\begin{equation}
\var\left(\hat{\alpha}\right) \geq \frac{1}{-\Ebb\left[\frac{\partial^2\ln p(\xbf;\alpha)}{\partial\alpha^2}\right]}
\end{equation}

\[
\Ebb\left[\frac{\partial^2\ln p(\xbf;\alpha)}{\partial\alpha^2}\right] = \Ebb\left[-\frac{m}{\alpha^2} + \frac{(m-n)}{(1-\alpha)^2}\right]
\]
\[
-\frac{n}{\alpha}+\frac{n\alpha}{(1-\alpha)^2)}-\frac{n}{(1-\alpha)^2)}=-\frac{n}{\alpha(1-\alpha)}
\]
\begin{equation}
\var\left(\hat{\alpha}\right) \geq \frac{\alpha(1-\alpha)}{n}\label{eq:crlb}
\end{equation}
Because the sample average attains this variance, we know that it is a \ac{MVU} estimator for $\alpha$. Although this result my seem inconsistent with our results above for \ac{AV} and \ac{PAV}, it is not. The reason is that an estimator can have variance no lower than (\ref{eq:crlb}) for the assumed \ac{PDF} in (\ref{eq:pdf}) - the assumed \ac{PDF} is different for the \ac{AV} and \ac{PAV} and hence the variance for an unbiased estimator may be lower.

\textbf{NOTE TO SELF}
\begin{quote}
I need to find the assumed \ac{PDF}s for \ac{AV} and \ac{PAV} before I can calculate the \ac{CRLB}.
\end{quote}

\section{Two Links}
We extend the previous analysis as before but now for the two link case.
\subsection{Monte Carlo Estimation}
We generate two sequences of unifrom random variables $U_{1}^{(n)}$ and $U_{2}^{(n)}$ and transforming them to Bernoulli random variables $X_{i}^{(n)} = h(U_{i}^{(n)}) = \mathbb{I}_{U_{i}^{(n)} < \alpha_i}$. The Monte Carlo estimator is then

\begin{equation}
\hat{\alpha}_{MC} = \frac{1}{n}\displaystyle\sum_{i=1}^{n}X_{1}^{(i)}X_{2}^{(i)}
\end{equation}

This is clearly unbiased and has a normalized variance of

\begin{equation}
n\var\left(\hat{\alpha}_{MC}\right) = \alpha_1\alpha_2(1-\alpha_1\alpha_2)
\end{equation}

\subsection{Antithetic Variables}
The antithetic variable estimator is now

\begin{equation}
\hat{\alpha}_{AV} = \frac{1}{n}\displaystyle\sum_{i=1}^{n/2}h(U_{1}^{(i)})h(U_{2}^{(i)})+h(1 - U_{1}^{(i)})h(1 - U_{2}^{(i)})
\end{equation}

As before, because expectation is linear and because $h(U_{i}^{(n)})$ and $h(1-U_{i}^{(n)})$ are identically distributed, it is easy enough to verify that the estimator is unbiased. For ease of notation, we replace $h(U_{i}^{(i)})h(U_{2}^{(i)})$ with $g(\Ubf^{(i)})$. The normalized variance is given as

\begin{align*}
n\var\left(\hat{\alpha}_{AV}\right) &= \frac{1}{n}\sum_{i=1}^{n/2}\var\left(g(\Ubf^{(i)}) + g(1-\Ubf^{(i)})\right)\\
&=\frac{\var\left(g(\Ubf^{(i)})\right) + \var\left(g(1-\Ubf^{(i)})\right) + 2\cov\left(g(\Ubf^{(i)}),g(1-\Ubf^{(i)})\right)}{2}\\
&=\var\left(g(\Ubf^{(i)})\right) + \cov\left(g(\Ubf^{(i)}),g(1-\Ubf^{(i)})\right)\\
&=\mathbb{E}\left[g(\Ubf^{(i)})^2\right] + \mathbb{E}\left[g(\Ubf^{(i)})g(1-\Ubf^{(i)})\right] - 2\mathbb{E}\left[g(\Ubf^{(i)})\right]^{2}\\
&=\alpha_1\alpha_2+ 4\mathbb{I}_{\alpha_1 > \frac{1}{2},\alpha_2 > \frac{1}{2}}(\alpha_1-\frac{1}{2})(\alpha_2 > \frac{1}{2})-2\alpha_1^{2}\alpha_2^2
\end{align*}

\subsection{Parameterized Antithetic Variables}
When we extend the proposed technique of parameterized antithetic variables, we obtain the following estimator
\begin{align}
\hat{\alpha}_{PAV} & = & \frac{1}{n} \sum_{i=1}^{n/2} \beta \left( \mathbf{1}_{U_{1}^{(i)} \leq \alpha_1,U_{2}^{(i)} \leq \alpha_2} + \mathbf{1}_{\beta-U_{1}^{(i)} \leq \alpha_1,\beta-U_{2}^{(i)} \leq \alpha_2} \right) +\\
&  & (1-\beta) \left( \mathbf{1}_{\bar{U}_{1}^{(i)} \leq \alpha_1,\bar{U}_{2}^{(i)} \leq \alpha_2} + \mathbf{1}_{1-\bar{U}_{1}^{(i)} + \beta < \alpha_1,1-\bar{U}_{2}^{(i)} + \beta < \alpha_2} \right)
\end{align}
Unfortunately, this is a biased estimator. The bias for this particular estimator is dependent upon both the choice of $\beta$ and $\alpha$ and so it can not be adjusted for.

\nocite{*}
\bibliographystyle{plain}
\bibliography{sources}
\end{document}